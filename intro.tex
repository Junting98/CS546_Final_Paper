Event detection (ED) aims to identify and classify different event triggers in a sentence into specific event types. For example, in the sentence \textit{"They killed by hostile fire in Iraqi"}, the \textit{"fire"} can be identified as an event trigger which can be classified as the \textit{"Attack"} event type.

Existing supervised event detection methods usually assumes that the ontology of the event types (also called event classes) is pre-defined and the number of event types is fixed. However, in the real word scenario, the event detection is usually continual, which means the ontology of event types can keep changing and expanding: new types of events and more fine-grained sub-types are constantly added to the system. Therefore. although recent years the supervised event detection methods have been put forward and showed promising performance \citep{chen2015event, du2020event, liu2020event, lu2021text2event}, they are not suitable for continual settings/learning (called called continual learning or lifelong learning or incremental learning)\citep{ring1994continual, thrun1998lifelong, cao2020incremental}. 

There are other problems need to be considered in the continual event detection.

The costs on the memory and computing can be dramatically high. A natural approach to address the continual event detection is to save the old data and re-train the whole model on the combination of old data and new data. However, the data stream for the continual event detection can be endless. Preserving the data for all the old event types demands extremely high cost on the storage. Also, re-training the model when the data come can be highly expensive on computation. 

Another problem for the continual event detection is the \textit{catastrophic forgetting}\citep{mccloskey1989catastrophic, french1999catastrophic}. Another intuitive method to handle the continual event detection problem is to finetune pre-trained models on new data. However, this method can cause catastrophic forgetting problem: after finetuning the models on new event types, the accuracy of identifying old types suffer from significant dropping. 

Much efforts have been tired to address the catastrophic forgetting problem. The major methods can be categorized into three classes: 
\begin{enumerate}[noitemsep]
  \item Significant parameters based methods: try to preserve important parameters of the models learned on the old classes when learning the new classes \citep{kirkpatrick2017overcoming, aljundi2018memory}.
  \item Rehearsal methods: combine older samples into the new data during training, by replaying older data such as \citet{rebuffi2017icarl, hou2019learning}, by utilizing generative models trained on older data such as \citet{shin2017continual}, by leveraging knowledge distillation that produces representations or targets from older predictors \citep{li2017learning, cao2020incremental}.
  \item Sparse representation methods, which leave room for future learning and mitigate the catastrophic forgetting issues \citep{liu2019utility, aljundi2018selfless}.
\end{enumerate}

Moreover, the distribution of event types is naturally imbalanced in the natural language. This is also called Long-tail distribution \citep{yu2021lifelong}. Previous methods \citep{nguyen2016two, cao2020incremental} tend to focus on the frequent event types which can gain low performance on rare event types. Predicting the events types with few data points is also called \textit{few-shot event detection}, and a method using \textit{Causal Intervention} was introduced to address the few-shot problem \citep{chen2021honey}.

Therefore, this paper proposes a method trying to address all the problems above:
\begin{itemize}[noitemsep]
  \item Learn new types and dynamic ontology when new data come;
  \item Reduce high cost on the computation and storage for systems;
  \item Avoid the catastrophic forgetting problem when learning new knowledge;
  \item Handle the long-tail distribution problem for event types.
\end{itemize}

Meta-learning based methods have achieved great success in various learning paradigms, i.e. few-shot learning, online learning and continual learning \citep{harrison2019continuous, javed2019meta}. They achieve state-of-the-art performance on tasks such as few-shot image classification. Because the meta-learning methods proved to be useful in: 
\begin{itemize}[noitemsep]
  \item Online learning -- learn new types and dynamic ontology when new data come;
  \item No need to save much old data -- reduce high cost on the computation and storage for systems;
  \item Continual learning -- avoid the catastrophic forgetting problem when learning new knowledge;
  \item Few-shot learning -- handle the long-tail distribution problem for event types.
\end{itemize}
this paper is motivated to explore a meta-learning framework that can be effectively applied to continual event detection setup and also address the long-tail problem. The main contribution of this paper: 
\begin{itemize}[noitemsep]
  \item We adapt the meta-learning framework based on \citet{javed2019meta} to get meta-representation to address the four major problems above in the continual event detection;
  \item Our base model results show significant improvements over both the fine-tune and pre-trained fine-tune baselines.;
  \item Our model-agnostic representation module can be used as pre-trained language model initialization for better performance
  \item Our framework can be easily adapted to other continual Information Extraction (IE) tasks.
\end{itemize}