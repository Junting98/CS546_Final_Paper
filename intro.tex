Event detection (ED) aims to identify and classify different event triggers in a sentence into specific event types. For example, in the sentence \textit{"They killed by hostile fire in Iraqi"}, the \textit{"fire"} can be identified as an event trigger which can be classified as the \textit{"Attack"} event type.

Existing supervised event detection methods usually assumes that the ontology of the event types (also called event classes) is pre-defined and the number of event types is fixed. However, in the real word scenario, the event detection is usually continual, which means the ontology of event types can keep changing and expanding: new types of events and more fine-grained sub-types are constantly added to the system. Therefore. although recent years the supervised event detection methods have been put forward and showed promising performance \citep{chen2015event, du2020event, liu2020event, lu2021text2event}, they are not suitable for continual settings/learning \citep{ring1994continual, thrun1998lifelong, cao2020incremental}. 

There are other problems in the continual event detection. The costs on the memory and computing can be dramatically high. A natural approach to address the continual event detection is to save the old data and re-train the whole model on the combination of old data and new data. However, the data stream for the continual event detection can be endless. Preserving the data for all the old event types demands extremely high cost on the storage. Also, re-training the model when the data come can be highly expensive on computation. 

Another problem for the continual event detection is the \textit{catastrophic forgetting}\citep{mccloskey1989catastrophic, french1999catastrophic}. An intuitive method to handle the continual event detection problem is to finetune the pre-trained models on new data. However, this method can cause catastrophic forgetting problem: after finetuning the models on new event types, the accuracy of identifying old types suffer from significant dropping. 

Much efforts have been tried to address the catastrophic forgetting problem. The major methods can be categorized into three classes: 
\begin{itemize}[noitemsep]
  \item Significant parameters based methods: try to preserve important parameters of the models learned on the old classes when learning the new classes \citep{kirkpatrick2017overcoming, aljundi2018memory}.
  \item Rehearsal methods: combine older samples into the new data during training, by replaying older data such as \citep{rebuffi2017icarl, hou2019learning}, by utilizing generative models trained on older data such as          \citep{shin2017continual}, by leveraging knowledge distillation that produces representations or targets from older predictors \citep{li2017learning, cao2020incremental}.
  \item Sparse representation methods, which leave room for future learning and mitigate the catastrophic forgetting issues \citep{liu2019utility, aljundi2018selfless}.
\end{itemize}


Moreover, the distribution of event types is naturally imbalanced in the natural language.  New events are inherently long-tail \citep{yu2021lifelong} as they usually have significantly fewer instances. Previous methods \citep{nguyen2016two, cao2020incremental} tend to have lower performance on rare event types.  

The objective of this project is to develop a framework that can:
\begin{itemize}[noitemsep]
\item Learn new types and dynamic ontology when new data come;
 \item Reduce high cost on the computation and storage for systems;
 \item Avoid the catastrophic forgetting problem when learning new knowledge;
\item Handle the long-tail distribution of new event types.
\end{itemize}

Meta-learning based methods have achieved great success in various learning paradigms, i.e. few-shot learning, online learning and continual learning \citep{harrison2019continuous, javed2019meta}. They achieve state-of-the-art performance on tasks such as few-shot image classification. 



Therefore, this paper is motivated to explore a meta-learning framework that can be effectively applied to continual event detection setup and also address the long-tail problem.  The main contribution of this paper:

\begin{itemize}[noitemsep]
\item We incorporate the long-tail problem of new event types and proposed a new event detection task: continual few-shot event detection.

  \item We developed a meta-learning based model-agnostic representation module can be used as pre-trained language model initialization for the continual few-shot event detection task.
  \item Our model shows significant performance gain over fine-tune based baselines and is on par with state-of-the-art continual event detection baselines.
\end{itemize}
