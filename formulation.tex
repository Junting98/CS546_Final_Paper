
\begin{figure*}[ht]
\centering
    \includegraphics[scale=0.6]{imgs/framework.jpg}
    \caption{Our proposed framework composed of two modules: Representation Module and Adaption Module}
    \label{img:1}
\end{figure*}

Event detection(ED) is a traditional task in natural language processing. Given a text sequence $w_{1:L}$ and a text span of $\{s, t\}$, We want to classify the given text span into an event type.  In a continuous event detection(CED) setup, the training and testing are separated into several stages, denoted as $t$. At each new stage$t$,   a steam of new data $\mathcal{D}^t = (X_1, Y_1), (X_2, Y_2), \dots, (X_ku Y_k), \dots$ arrives.  In this project, all data in $\mathcal{D}^t $ has the same classes, denoted as $C^{t}$.  $C^t \cap C^{t'}  = \emptyset$, meaning that at each stage,  the model will always receive a stream of data from unseen event types.  At each time stage $t$, the event detection model uses $\mathcal{D}^t $ to update its parameters, and the updated model should achieve good performance on all previously observed event classes.  Therefore, at each stage, we test the model on $\cup_{i=1}^t \mathcal{D}^i$, and the model should give a prediction from  $\cup_{i=1}^t C^i$. 

There are usually few instances of newly seen event types in a real-world scenario, making the new events inherently long-tailed. Therefore, in this project, we simulate such long-tail distribution of new events by formulating the CED problem as a continual few-shot event detection problem. 


%At first, introduce some Automatic Content Extraction (ACE) terminologies to help understand the Event Detection tasks \citep{cao2020incremental}: "\textbf{Event trigger} refers to a word that most clearly expresses the occurrence of an event. \textbf{Event arguments} are participants of the event. \textbf{Event mention} refers to a phrase or sentence within which an event is described."
%
%Because Continual Event Detection (CED) in Natural Language problem can be viewed as a kind of Continual Learning Prediction (CLP), this paper, which majorly exploits meta-learning based methods, formulate the CED problem based on the CLP problem formulation from \citet{javed2019meta}.
%A Continual Event Detection (CED) in Natural Language problem consists of an endless data stream:
%\[\mathcal{T} = (X_1, Y_1), (X_2, Y_2), \dots, (X_t, Y_t), \dots\]
%for inputs $X_t$ (event mentions) and targets $Y_t$ (event types), from sets $\mathcal{X}$ and $\mathcal{Y}$. The random vector $Y_t$ is sampled
%according to an unknown distribution $p(Y|X_t)$. We define $S_k = (X_j+1, Y_j+1), (X_j+2, Y_j+2), \dots, (X_j+k, Y_j+k)$ as a trajectory of length k which is smapled randomly from the CET problem $\mathcal{T}$, and $p(S_k|\mathcal{T})$ represents a distribution over all the trajectories $S_k$.
%
%Our goal is to learn a function $f_W,\theta$ which predict $Y_t$ for $X_t$, and minimize the loss function below for a CED problem:
%\begin{equation}
%    \begin{split}
%    &\mathcal{L}_{C E D}(W, \theta)  \\
%    &\stackrel{\text { def }}{=}  \mathbb{E}\left[\ell\left(f_{W, \theta}(X), Y\right)\right] \\ 
%    & =\int\left[\int \ell\left(f_{W, \theta}(x), y\right) p(y \mid x) d y\right] \mu(x) d x
%    \end{split}
%\end{equation}
%where $W$ and $\theta$ are the parameters to be updated to minimize the objective $\mathcal{L}_{C E D}$.
